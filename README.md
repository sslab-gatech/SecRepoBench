# üõ°Ô∏è SecRepoBench
### **Links:**
- Paper: [SecRepoBench: Benchmarking Code Agents for Secure Code Completion in Real-World Repositories](https://arxiv.org/abs/2504.21205)
- Website & Leaderboard: https://stevenshen3641.github.io/projects/secrepobench/

## üìù Overview

SecRepoBench is a repository-level secure code completion benchmark. It contains 318 code completion tasks obtained from 27 popular GitHub C/C++ repositories covering 15 CWEs. Our benchmark can be used to evaluate both standalone LLMs with a context retriever and agent frameworks with access to the entire repository, which gives a comprehensive assessment of different code generation paradigms.

## üí° Framework

![Framework diagram](./assets/framework.png)

Each code completion task takes a target function with a masked region and the entire repository providing context as inputs to either a standalone LLM with a context retriever or an agent framework, which then generates code to fill the empty region. The generated code is compiled with the full repository and evaluated on two dimensions: correctness using developer-written unit tests and security using Proof-of-Concept exploits from OSS-Fuzz.

For standalone LLM evaluation, SecRepoBench supports three context retrieval methods: `BM25`, `dense-file`, and `in-file`. In the paper, we use `BM25` as the default context retriever, which retrieves the top 5 most relevant functions from the repository as the context.

For agent evaluation, we run Aider and OpenHands inside the ARVO container to ensure the environment provides all necessary dependencies to compile the task codebase. This setup gives agents access to the complete repository environment, including all required build systems, dependencies, and testing frameworks. For Claude Code, though we could not run it inside the container due to compatibility issues, we clone the task codebase locally as Claude Code's working directory.

SecRepoBench supports four prompt types: `no-security-reminder`, `sec-generic`, `sec-specific`, and `security-policy`.

- `no-security-reminder`: this prompt does not give the LLM any reminders to generate secure code.
- `sec-generic`: this prompt tells the LLM that it is a security expert, and asks the LLM to ensure that the generated code is secure.
- `sec-specific`: this prompt tells the LLM that it is a security expert. It then asks the LLM to ensure that the code does not contain the specific CWE present in the developer-written, pre-patched code, and provides the MITRE description of that CWE.
- `security-policy`: this prompt provides the LLM with task-specific instructions generated by GPT-4o on how to avoid the CWE present in the developer-written, pre-patched code. This prompt is based on the optional security policy in [SecCodePLT](https://seccodeplt.github.io/).

In the paper, we use `no-security-reminder` as the default prompt type to reflect realistic code completion scenarios where developers do not know beforehand what vulnerabilities might be introduced, requiring models to identify and prevent vulnerabilities solely by understanding the task context.

## ‚öôÔ∏è Configuration

### **1. Install `uv`**

Please check [installation methods](https://docs.astral.sh/uv/getting-started/installation/) to install `uv` on your platform.

### **2. Install dependencies**

Run the following command to install dependencies required by SecRepoBench:

```bash
cd SecRepoBench
uv sync
```

To install dependencies for agent framework `ClaudeCode`, please run the following commands:

```bash
curl -fsSL https://claude.ai/install.sh | bash
```

### **3. Set Environment Variables**

SecRepoBench requires API keys for the language models you plan to use. Please set the following environment variables:

```bash
export OPENAI_API_KEY=<YOUR_API_KEY>
export ANTHROPIC_API_KEY=<YOUR_API_KEY>
export GEMINI_API_KEY=<YOUR_API_KEY>
export TOGETHER_API_KEY=<YOUR_API_KEY>
```


### **4. Extract metadata files**
Upzip metadata that would be used during inference or evalutaion:

```bash
gunzip -k report.json.gz sample_metadata.json.gz
```

## üöÄ Running Inference

To run inference using SecRepoBench:

```bash
uv run run_inference.py \
  --agents [YOUR_AGENT_NAMES] \
  --model-names [YOUR_MODEL_NAMES] \
  --prompt-types [YOUR_PROMPT_TYPES] \
  --context-types [YOUR_CONTEXT_TYPES] \
  [--rerun]
```

- **Agent names**:
  - `none` (Without using agent framework)
  - `aider`
  - `openhands`
  - `claudecode`
- **Model names**: Defined in `assets/constants.py`
- **Prompt types**:
  - `no-security-reminder`
  - `sec-generic`
  - `sec-specific`
  - `security-policy`
- **Context types**: (This option is disabled while using agent framework)
  - `BM25`
  - `dense-file`
  - `in-file`

üìÅ *Code completions are saved in the `completions/` directory.*


## üìä Running Evaluation

To evaluate the model completions:

```bash
uv run run_eval.py \
  --agents [YOUR_AGENT_NAMES] \
  --model-names [YOUR_MODEL_NAMES] \
  --prompt-types [YOUR_PROMPT_TYPES] \
  --context-types [YOUR_CONTEXT_TYPES] \
  [--rerun]
```

üìÅ *Evaluation results are saved in the `eval_results/` directory.*

## üìñ Citation
```latex
@article{shen2025secrepobench,
  title={SecRepoBench: Benchmarking Code Agents for Secure Code Completion in Real-World Repositories},
  author={Shen, Chihao and Dilgren, Connor and Chiniya, Purva and Griffith, Luke and Ding, Yu and Chen, Yizheng},
  journal={arXiv preprint arXiv:2504.21205},
  year={2025}
}
```